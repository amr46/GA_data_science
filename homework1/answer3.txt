Question 3
Explain your approach and methodologies, including a brief explanation of
KNN, Bayes Classification, and cross validation.

Answer:

Brief description of KNN, Naive Bayes and Cross validation

We use K-Nearest Neighbor and Naive Bayes classification to build models to fit this data.

K-Nearest Neighbor is a simple classification algorithm that classifies a data point based on its k-nearest data points. K \
is varied to try to come up with the optimum K that minimizes classification error of the model.

Naive Bayes classifiers are probabalistic models that attempt to classify a data point assuming that all variables used for this classification are purely independent of each other.

We attempt to build models so as to minimize the overall classification error by these models. These framework\
s are applicable to our data set because we are solving a classification problem - does an individual pose a good risk, or a bad risk.

The models will be trained on subsets of the data set. Then, the remainder of \
the data set will be used as a test set to see how well the model did.

Then different chunks of data will be chosen and new models built and scored against the complementary sets of\
 data in a process known as N-fold cross validation. Here N represents the number of iterations of partitionin\
g the master data set into training and test sets.

The point of N-fold cross validation is to make sure that the errors from models generated by the algorithms a\
re considered when being trained on different sections of the data set.

We will randomize the data rows to ensure an even distribution.
---------

Approach:

KNN 
- Used 5-Fold cross validation. 
- Chunked the data set into 5 train/test sets. 
- Each test set contained 800 training rows, and 200 test rows. 
- Used different sets of 200 rows each time as the test set
- Used each row in exactly 1 test set, i.e. rows were not repeated in test sets.
- Used a constant RNG seed to randomize the numbers from 1...1000. 
- Then took lines at indices 1-200, 201-400, ...801-1000 as the test set,
  and the complementary lines as the training set.  
- Considered values of k from 1-35 after which the deterioration in error was not worth considering
- Error was computed by comparing the number of misclassified examples to the total number of examples
  in the test set.


Naive Bayes
- Built a Naive Bayes classifier, and used N-fold cross validation as above. 
- N was varied from 10,5,3,2 in each case using 100, 200, 333, 500 rows for the test set. 
- Randomization of the data set and choosing train and test sets was done the same way as in KNN
- Error was manually computed by comparing the number of misclassified examples to the total number of examples 
  in the test set. 
 




